{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Excalibro1/zimwithloracolab/blob/main/Zimageturboloraqwenb8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Credit:\n",
        "[Z-Image Github](https://github.com/Tongyi-MAI/Z-Image) <br>\n",
        "Colab Code: [camenduru](https://github.com/camenduru/Z-Image-jupyter\n",
        "\n",
        "Edited from camemduru and added lora support and downloads the qwen_3_4b.safetensors also the Qwen3-4B-abliterated.Q8_0.gguf with an option in the ui to switch between them"
      ],
      "metadata": {
        "id": "PROsUqH9x77h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title üîß Install Z-Image Turbo + GGUF (Drive + cache aware)\n",
        "import os, pathlib, subprocess, shutil\n",
        "\n",
        "print(\"üìå Starting Z-Image Turbo setup...\")\n",
        "\n",
        "############################\n",
        "# 0Ô∏è‚É£ Mount Drive EARLY\n",
        "############################\n",
        "from google.colab import drive\n",
        "if not os.path.exists(\"/content/drive/MyDrive\"):\n",
        "    print(\"üîå Mounting Google Drive...\")\n",
        "    drive.mount(\"/content/drive\")\n",
        "else:\n",
        "    print(\"üíæ Drive already mounted.\")\n",
        "\n",
        "############################\n",
        "# 1Ô∏è‚É£ Folder layout\n",
        "############################\n",
        "COMFY_ROOT = \"/content/ComfyUI\"\n",
        "CACHE_ROOT = \"/content/ZImage_ComfyUI_cache\"\n",
        "DRIVE_ROOT = \"/content/drive/MyDrive/ZImage_ComfyUI/models\"\n",
        "\n",
        "os.makedirs(CACHE_ROOT, exist_ok=True)\n",
        "os.makedirs(DRIVE_ROOT, exist_ok=True)\n",
        "\n",
        "print(f\"üìÇ Cache folder:  {CACHE_ROOT}\")\n",
        "print(f\"üìÇ Drive folder:  {DRIVE_ROOT}\")\n",
        "\n",
        "# Base model subfolders (cache + Drive)\n",
        "base_subdirs = [\"diffusion_models\", \"clip\", \"vae\"]\n",
        "for s in base_subdirs:\n",
        "    os.makedirs(os.path.join(CACHE_ROOT, s), exist_ok=True)\n",
        "    os.makedirs(os.path.join(DRIVE_ROOT, s), exist_ok=True)\n",
        "\n",
        "# LoRAs live only on Drive\n",
        "os.makedirs(os.path.join(DRIVE_ROOT, \"loras\"), exist_ok=True)\n",
        "\n",
        "############################\n",
        "# 2Ô∏è‚É£ Install system + Python deps\n",
        "############################\n",
        "print(\"‚¨áÔ∏è Installing dependencies...\")\n",
        "%cd /content\n",
        "!apt -y install aria2\n",
        "\n",
        "# CUDA-matched PyTorch first\n",
        "!pip install torch==2.9.0+cu126 torchvision==0.24.0+cu126 torchaudio==2.9.0+cu126 --index-url https://download.pytorch.org/whl/cu126\n",
        "\n",
        "# Core runtime deps\n",
        "!pip install -q av==16.0.1 torchsde==0.2.6 safetensors pillow scipy tqdm einops transformers pyyaml aiohttp\n",
        "\n",
        "# ComfyUI supporting dependencies\n",
        "!pip install -q comfyui-frontend-package==1.33.13\n",
        "!pip install -q comfyui-workflow-templates==0.7.54\n",
        "!pip install -q comfyui-embedded-docs==0.3.1\n",
        "!pip install -q spandrel==0.4.1 SQLAlchemy==2.0.44\n",
        "!pip install -q pydantic==2.12.3 pydantic-settings==2.12.0\n",
        "\n",
        "############################\n",
        "# 3Ô∏è‚É£ Install / update ComfyUI backend\n",
        "############################\n",
        "if not os.path.exists(COMFY_ROOT):\n",
        "    print(\"‚¨áÔ∏è Cloning ComfyUI...\")\n",
        "    !git clone https://github.com/comfyanonymous/ComfyUI.git\n",
        "else:\n",
        "    print(\"‚úîÔ∏è ComfyUI already exists\")\n",
        "\n",
        "%cd /content/ComfyUI\n",
        "# Make sure internal deps are installed too\n",
        "!pip install -q -r requirements.txt\n",
        "\n",
        "############################\n",
        "# 4Ô∏è‚É£ Install / repair ComfyUI-GGUF nodes\n",
        "############################\n",
        "CUSTOM_NODES_ROOT = os.path.join(COMFY_ROOT, \"custom_nodes\")\n",
        "os.makedirs(CUSTOM_NODES_ROOT, exist_ok=True)\n",
        "%cd \"$CUSTOM_NODES_ROOT\"\n",
        "\n",
        "gguf_path = os.path.join(CUSTOM_NODES_ROOT, \"ComfyUI-GGUF\")\n",
        "\n",
        "# If folder exists but looks broken (no 'nodes' dir), reset it\n",
        "if os.path.exists(gguf_path) and not os.path.isdir(os.path.join(gguf_path, \"nodes\")):\n",
        "    print(\"‚ö†Ô∏è ComfyUI-GGUF folder looks corrupted ‚Üí resetting...\")\n",
        "    !rm -rf \"ComfyUI-GGUF\"\n",
        "\n",
        "if not os.path.exists(gguf_path):\n",
        "    print(\"‚¨áÔ∏è Installing ComfyUI-GGUF nodes...\")\n",
        "    !git clone https://github.com/city96/ComfyUI-GGUF.git\n",
        "    !pip install -q gguf\n",
        "else:\n",
        "    print(\"‚úîÔ∏è GGUF nodes already available\")\n",
        "\n",
        "############################\n",
        "# 5Ô∏è‚É£ Required model inventory\n",
        "############################\n",
        "required_files = {\n",
        "    \"diffusion_models\": {\n",
        "        \"z-image-turbo-fp8-e4m3fn.safetensors\":\n",
        "        \"https://huggingface.co/T5B/Z-Image-Turbo-FP8/resolve/main/z-image-turbo-fp8-e4m3fn.safetensors\"\n",
        "    },\n",
        "    \"clip\": {\n",
        "        # Standard CLIP\n",
        "        \"qwen_3_4b.safetensors\":\n",
        "        \"https://huggingface.co/T5B/Z-Image-Turbo-FP8/resolve/main/qwen_3_4b.safetensors\",\n",
        "\n",
        "        # GGUF CLIP\n",
        "        \"Qwen3-4B-abliterated.Q8_0.gguf\":\n",
        "        \"https://huggingface.co/mradermacher/Qwen3-4B-abliterated-GGUF/resolve/main/Qwen3-4B-abliterated.Q8_0.gguf\"\n",
        "    },\n",
        "    \"vae\": {\n",
        "        \"ae.safetensors\":\n",
        "        \"https://huggingface.co/T5B/Z-Image-Turbo-FP8/resolve/main/ae.safetensors\"\n",
        "    }\n",
        "}\n",
        "\n",
        "############################\n",
        "# 6Ô∏è‚É£ Smart copy + download into CACHE\n",
        "############################\n",
        "for subfolder, files in required_files.items():\n",
        "    print(f\"\\nüîé Checking {subfolder}...\")\n",
        "    drive_path = pathlib.Path(DRIVE_ROOT) / subfolder\n",
        "    cache_path = pathlib.Path(CACHE_ROOT) / subfolder\n",
        "\n",
        "    for filename, url in files.items():\n",
        "        src = drive_path / filename\n",
        "        dst = cache_path / filename\n",
        "\n",
        "        if dst.exists():\n",
        "            print(f\"‚úîÔ∏è Cache OK: {filename}\")\n",
        "            continue\n",
        "\n",
        "        if src.exists():\n",
        "            print(f\"üì• Copying from Drive ‚Üí Cache: {filename}\")\n",
        "            shutil.copy2(src, dst)\n",
        "            continue\n",
        "\n",
        "        print(f\"‚¨áÔ∏è Missing, downloading: {filename}\")\n",
        "        cmd = [\n",
        "            \"aria2c\", \"--console-log-level=error\", \"-c\",\n",
        "            \"-x\", \"16\", \"-s\", \"16\", \"-k\", \"1M\",\n",
        "            url, \"-d\", str(cache_path), \"-o\", filename\n",
        "        ]\n",
        "        subprocess.run(cmd, check=True)\n",
        "\n",
        "        if dst.exists():\n",
        "            print(f\"   ‚ûï Downloaded: {filename}\")\n",
        "        else:\n",
        "            print(f\"   ‚ùå FAILED download: {filename}\")\n",
        "\n",
        "############################\n",
        "# 7Ô∏è‚É£ Summary\n",
        "############################\n",
        "print(\"\\nüéØ Setup complete! Final locations:\")\n",
        "print(f\"  Base models (CACHE) ‚Üí {CACHE_ROOT}\")\n",
        "print(f\"  LoRAs (DRIVE)       ‚Üí {DRIVE_ROOT}/loras\")\n",
        "\n",
        "print(\"\\nüìÅ Cache tree:\")\n",
        "for root, dirs, files in os.walk(CACHE_ROOT):\n",
        "    level = root.replace(CACHE_ROOT, \"\").count(os.sep)\n",
        "    indent = \"  \" * level\n",
        "    print(f\"{indent}{os.path.basename(root)}/\")\n",
        "    subindent = \"  \" * (level + 1)\n",
        "    for f in files:\n",
        "        print(f\"{subindent}{f}\")"
      ],
      "metadata": {
        "id": "nWezqa_wtcor",
        "cellView": "form",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title üñº Z-Image Turbo ‚Äì LoRA + optional GGUF CLIP (cached UNet, auto low-RAM, memory monitor)\n",
        "import os, sys, gc, random, contextlib\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import gradio as gr\n",
        "import requests\n",
        "import psutil  # for system RAM stats\n",
        "\n",
        "# Optional small speed hint\n",
        "torch.set_float32_matmul_precision(\"high\")\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "# ---------- Paths ----------\n",
        "COMFY_ROOT          = \"/content/ComfyUI\"\n",
        "BASE_CACHE          = \"/content/ZImage_ComfyUI_cache\"\n",
        "DRIVE_MODELS_ROOT   = \"/content/drive/MyDrive/ZImage_ComfyUI/models\"\n",
        "CACHE_DIFF_DIR      = os.path.join(BASE_CACHE, \"diffusion_models\")\n",
        "CACHE_CLIP_DIR      = os.path.join(BASE_CACHE, \"clip\")\n",
        "CACHE_VAE_DIR       = os.path.join(BASE_CACHE, \"vae\")\n",
        "DRIVE_LORAS_DIR     = os.path.join(DRIVE_MODELS_ROOT, \"loras\")\n",
        "os.makedirs(DRIVE_LORAS_DIR, exist_ok=True)\n",
        "\n",
        "# ---------- Sanity ----------\n",
        "if not os.path.exists(os.path.join(COMFY_ROOT, \"folder_paths.py\")):\n",
        "    raise RuntimeError(\"‚ùå ComfyUI not found ‚Äî run the install/setup cell first.\")\n",
        "\n",
        "%cd /content/ComfyUI\n",
        "\n",
        "if COMFY_ROOT not in sys.path:\n",
        "    sys.path.append(COMFY_ROOT)\n",
        "CUSTOM_NODES_ROOT = os.path.join(COMFY_ROOT, \"custom_nodes\")\n",
        "if CUSTOM_NODES_ROOT not in sys.path:\n",
        "    sys.path.append(CUSTOM_NODES_ROOT)\n",
        "\n",
        "# Alias for GGUF node package\n",
        "GGUF_SRC   = os.path.join(CUSTOM_NODES_ROOT, \"ComfyUI-GGUF\")\n",
        "GGUF_ALIAS = os.path.join(CUSTOM_NODES_ROOT, \"ComfyUI_GGUF\")\n",
        "\n",
        "if os.path.isdir(GGUF_SRC) and not os.path.exists(GGUF_ALIAS):\n",
        "    try:\n",
        "        os.symlink(GGUF_SRC, GGUF_ALIAS)\n",
        "        print(f\"üîó Created alias package: {GGUF_ALIAS} ‚Üí {GGUF_SRC}\")\n",
        "    except FileExistsError:\n",
        "        pass\n",
        "\n",
        "import folder_paths\n",
        "from nodes import (\n",
        "    UNETLoader, CLIPLoader, VAELoader,\n",
        "    CLIPTextEncode, KSampler, EmptyLatentImage, VAEDecode,\n",
        "    LoraLoader,\n",
        ")\n",
        "import comfy.model_management as mm\n",
        "\n",
        "# ---------- GGUF availability ----------\n",
        "HAS_GGUF = False\n",
        "try:\n",
        "    from ComfyUI_GGUF.nodes import CLIPLoaderGGUF\n",
        "    HAS_GGUF = True\n",
        "    print(\"‚úÖ ComfyUI-GGUF: CLIPLoaderGGUF available (via alias package).\")\n",
        "except Exception as e:\n",
        "    print(\"‚ö†Ô∏è ComfyUI-GGUF CLIPLoaderGGUF not available:\", e)\n",
        "\n",
        "# ---------- Register model folders ----------\n",
        "folder_paths.add_model_folder_path(\"diffusion_models\", CACHE_DIFF_DIR)\n",
        "folder_paths.add_model_folder_path(\"clip\",              CACHE_CLIP_DIR)\n",
        "folder_paths.add_model_folder_path(\"text_encoders\",     CACHE_CLIP_DIR)\n",
        "folder_paths.add_model_folder_path(\"vae\",               CACHE_VAE_DIR)\n",
        "folder_paths.add_model_folder_path(\"loras\",             DRIVE_LORAS_DIR)\n",
        "\n",
        "get_dev = getattr(mm, \"get_torch_device\", None)\n",
        "DEFAULT_DEVICE = get_dev() if callable(get_dev) else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"üñ• Default device:\", DEFAULT_DEVICE)\n",
        "\n",
        "# ---------- Global cached UNet ----------\n",
        "GLOBAL_UNET = None\n",
        "GLOBAL_UNET_DEVICE = None\n",
        "GLOBAL_UNET_NAME = \"z-image-turbo-fp8-e4m3fn.safetensors\"\n",
        "GLOBAL_UNET_DTYPE = \"fp8_e4m3fn_fast\"\n",
        "\n",
        "# ---------- Memory helpers ----------\n",
        "def format_bytes(num: int) -> str:\n",
        "    return f\"{num / (1024 ** 3):.2f} GB\"\n",
        "\n",
        "def get_memory_snapshot():\n",
        "    snap = {}\n",
        "    vm = psutil.virtual_memory()\n",
        "    snap[\"ram_used\"] = vm.used\n",
        "    snap[\"ram_total\"] = vm.total\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        try:\n",
        "            free, total = torch.cuda.mem_get_info()\n",
        "            snap[\"vram_used\"] = total - free\n",
        "            snap[\"vram_total\"] = total\n",
        "        except Exception:\n",
        "            snap[\"vram_used\"] = torch.cuda.memory_allocated()\n",
        "            snap[\"vram_total\"] = 0\n",
        "        try:\n",
        "            snap[\"vram_peak\"] = torch.cuda.max_memory_allocated()\n",
        "        except Exception:\n",
        "            snap[\"vram_peak\"] = snap.get(\"vram_used\", 0)\n",
        "    return snap\n",
        "\n",
        "def log_memory(tag: str, log):\n",
        "    snap = get_memory_snapshot()\n",
        "    if \"vram_used\" in snap:\n",
        "        log(\n",
        "            f\"üìä {tag}: \"\n",
        "            f\"RAM={format_bytes(snap['ram_used'])}/{format_bytes(snap['ram_total'])} | \"\n",
        "            f\"VRAM={format_bytes(snap['vram_used'])}/{format_bytes(snap['vram_total'])} \"\n",
        "            f\"(peak {format_bytes(snap.get('vram_peak', 0))})\"\n",
        "        )\n",
        "    else:\n",
        "        log(\n",
        "            f\"üìä {tag}: \"\n",
        "            f\"RAM={format_bytes(snap['ram_used'])}/{format_bytes(snap['ram_total'])} | \"\n",
        "            f\"VRAM=N/A (CPU only)\"\n",
        "        )\n",
        "    return snap\n",
        "\n",
        "def memory_color(used: int, total: int):\n",
        "    if total <= 0:\n",
        "        return \"gray\"\n",
        "    frac = used / total\n",
        "    if frac < 0.5:\n",
        "        return \"green\"\n",
        "    elif frac < 0.8:\n",
        "        return \"orange\"\n",
        "    else:\n",
        "        return \"red\"\n",
        "\n",
        "def build_memory_status_html(snap: dict) -> str:\n",
        "    if not snap:\n",
        "        return \"<span style='font-family:monospace;font-size:0.76rem;'>No memory data yet.</span>\"\n",
        "\n",
        "    ram_used  = snap.get(\"ram_used\", 0)\n",
        "    ram_total = snap.get(\"ram_total\", 0)\n",
        "    ram_col   = memory_color(ram_used, ram_total)\n",
        "\n",
        "    html = [\n",
        "        \"<div style='font-family:monospace;font-size:0.76rem;line-height:1.4;'>\",\n",
        "        f\"<div><b>RAM:</b> \"\n",
        "        f\"<span style='color:{ram_col};'>{format_bytes(ram_used)} / {format_bytes(ram_total)}</span></div>\",\n",
        "    ]\n",
        "\n",
        "    if \"vram_used\" in snap and \"vram_total\" in snap:\n",
        "        vram_used  = snap.get(\"vram_used\", 0)\n",
        "        vram_total = snap.get(\"vram_total\", 0)\n",
        "        vram_peak  = snap.get(\"vram_peak\", 0)\n",
        "        vram_col   = memory_color(vram_used, vram_total)\n",
        "        html.append(\n",
        "            f\"<div><b>VRAM:</b> \"\n",
        "            f\"<span style='color:{vram_col};'>{format_bytes(vram_used)} / {format_bytes(vram_total)}</span></div>\"\n",
        "        )\n",
        "        html.append(\n",
        "            f\"<div><b>VRAM peak (this run):</b> {format_bytes(vram_peak)}</div>\"\n",
        "        )\n",
        "    else:\n",
        "        html.append(\"<div><b>VRAM:</b> N/A (CPU-only)</div>\")\n",
        "\n",
        "    html.append(\"</div>\")\n",
        "    return \"\\n\".join(html)\n",
        "\n",
        "# ---------- Cleanup ----------\n",
        "def hard_cleanup_models(log=None, keep_unet: bool = False):\n",
        "    \"\"\"\n",
        "    Aggressive cleanup, but optionally keep the cached UNet in VRAM.\n",
        "    \"\"\"\n",
        "    for name in (\"unload_all_loras\", \"cleanup_models\", \"unload_all_models\", \"soft_empty_cache\"):\n",
        "        if keep_unet and name == \"unload_all_models\":\n",
        "            if log:\n",
        "                log(\"üß† Skipping unload_all_models() to keep UNet cached.\")\n",
        "            continue\n",
        "\n",
        "        fn = getattr(mm, name, None)\n",
        "        if callable(fn):\n",
        "            try:\n",
        "                fn()\n",
        "                if log:\n",
        "                    log(f\"üß† {name}()\")\n",
        "            except Exception as e:\n",
        "                if log:\n",
        "                    log(f\"‚ö†Ô∏è {name} failed: {e}\")\n",
        "\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "# ---------- LoRA / CLIP helpers ----------\n",
        "def list_loras():\n",
        "    try:\n",
        "        files = folder_paths.get_filename_list(\"loras\")\n",
        "        files = [f for f in files if f.lower().endswith(\".safetensors\")]\n",
        "    except Exception:\n",
        "        if not os.path.isdir(DRIVE_LORAS_DIR):\n",
        "            return [\"<none>\"]\n",
        "        files = [\n",
        "            f for f in os.listdir(DRIVE_LORAS_DIR)\n",
        "            if f.lower().endswith(\".safetensors\")\n",
        "        ]\n",
        "    return [\"<none>\"] + sorted(files)\n",
        "\n",
        "def scan_clip_files(use_gguf: bool):\n",
        "    if not os.path.isdir(CACHE_CLIP_DIR):\n",
        "        return []\n",
        "    exts = (\".gguf\",) if use_gguf else (\".safetensors\",)\n",
        "    files = [\n",
        "        f for f in os.listdir(CACHE_CLIP_DIR)\n",
        "        if f.lower().endswith(exts)\n",
        "    ]\n",
        "    return sorted(files)\n",
        "\n",
        "def update_clip_choices(use_gguf: bool):\n",
        "    files = scan_clip_files(use_gguf)\n",
        "    if not files:\n",
        "        return gr.update(choices=[], value=None)\n",
        "    default = \"qwen_3_4b.safetensors\" if (not use_gguf and \"qwen_3_4b.safetensors\" in files) else files[0]\n",
        "    return gr.update(choices=files, value=default)\n",
        "\n",
        "# ---------- LoRA Downloader ----------\n",
        "def download_lora_from_ui(url, filename, hf_token, civitai_token):\n",
        "    logs = []\n",
        "    def log(msg):\n",
        "        msg = str(msg)\n",
        "        logs.append(msg)\n",
        "        print(msg)\n",
        "\n",
        "    url = (url or \"\").strip()\n",
        "    filename = (filename or \"\").strip()\n",
        "    hf_token = (hf_token or \"\").strip()\n",
        "    civitai_token = (civitai_token or \"\").strip()\n",
        "\n",
        "    if not url:\n",
        "        log(\"‚ùå No URL provided.\")\n",
        "        return gr.update(choices=list_loras(), value=\"<none>\"), \"\\n\".join(logs)\n",
        "\n",
        "    if not filename:\n",
        "        base = os.path.basename(url.split(\"?\")[0])\n",
        "        filename = base or \"lora_download.safetensors\"\n",
        "\n",
        "    if not filename.lower().endswith(\".safetensors\"):\n",
        "        filename += \".safetensors\"\n",
        "\n",
        "    dest = os.path.join(DRIVE_LORAS_DIR, filename)\n",
        "    os.makedirs(DRIVE_LORAS_DIR, exist_ok=True)\n",
        "\n",
        "    log(f\"üìÅ LoRA save path: {dest}\")\n",
        "    if os.path.exists(dest):\n",
        "        log(\"‚ÑπÔ∏è File already exists, overwriting‚Ä¶\")\n",
        "\n",
        "    headers = {}\n",
        "    lower_url = url.lower()\n",
        "\n",
        "    if \"huggingface.co\" in lower_url and hf_token:\n",
        "        headers[\"Authorization\"] = f\"Bearer {hf_token}\"\n",
        "        log(\"üîê Using HuggingFace token.\")\n",
        "\n",
        "    if \"civitai.com\" in lower_url and civitai_token:\n",
        "        headers[\"Authorization\"] = f\"Bearer {civitai_token}\"\n",
        "        headers[\"X-Api-Key\"] = civitai_token\n",
        "        log(\"üîê Using Civitai token.\")\n",
        "\n",
        "    try:\n",
        "        log(\"‚¨áÔ∏è Downloading LoRA...\")\n",
        "        with requests.get(url, headers=headers, stream=True) as r:\n",
        "            r.raise_for_status()\n",
        "            with open(dest, \"wb\") as f:\n",
        "                for chunk in r.iter_content(chunk_size=8192):\n",
        "                    if chunk:\n",
        "                        f.write(chunk)\n",
        "        log(\"‚úÖ Download complete.\")\n",
        "    except Exception as e:\n",
        "        log(f\"‚ùå Download failed: {e}\")\n",
        "        return gr.update(choices=list_loras(), value=\"<none>\"), \"\\n\".join(logs)\n",
        "\n",
        "    loras = list_loras()\n",
        "    new_value = filename if filename in loras else \"<none>\"\n",
        "    return gr.update(choices=loras, value=new_value), \"\\n\".join(logs)\n",
        "\n",
        "# ---------- Core generation ----------\n",
        "def generate_image(\n",
        "    prompt,\n",
        "    negative,\n",
        "    steps,\n",
        "    cfg,\n",
        "    sampler_name,\n",
        "    scheduler,\n",
        "    width,\n",
        "    height,\n",
        "    seed,\n",
        "    use_lora,\n",
        "    selected_lora,\n",
        "    lora_strength_model,\n",
        "    lora_strength_clip,\n",
        "    use_gguf_clip,\n",
        "    selected_clip_name,\n",
        "):\n",
        "    logs = []\n",
        "    def log(x):\n",
        "        x = str(x)\n",
        "        logs.append(x)\n",
        "        print(x)\n",
        "\n",
        "    # Reset peak VRAM for this run\n",
        "    if torch.cuda.is_available():\n",
        "        try:\n",
        "            torch.cuda.reset_peak_memory_stats()\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    mem_snapshot = log_memory(\"Before load\", log)\n",
        "\n",
        "    if not prompt or not prompt.strip():\n",
        "        log(\"‚ùå Empty prompt\")\n",
        "        status_html = build_memory_status_html(mem_snapshot)\n",
        "        return [], \"\\n\".join(logs), status_html\n",
        "\n",
        "    seed = int(seed if seed and seed >= 0 else random.randint(0, 2**31 - 1))\n",
        "    torch.manual_seed(seed)\n",
        "    log(f\"üî¢ Seed: {seed}\")\n",
        "\n",
        "    run_device = DEFAULT_DEVICE\n",
        "    use_cuda = (run_device == \"cuda\" and torch.cuda.is_available())\n",
        "    log(f\"üß† Device: {run_device} | GGUF CLIP: {use_gguf_clip}\")\n",
        "\n",
        "    # ----- UNet (cached) -----\n",
        "    global GLOBAL_UNET, GLOBAL_UNET_DEVICE\n",
        "    try:\n",
        "        if GLOBAL_UNET is None:\n",
        "            log(\"üì¶ Loading UNet (first time)...\")\n",
        "            GLOBAL_UNET = UNETLoader().load_unet(\n",
        "                GLOBAL_UNET_NAME,\n",
        "                GLOBAL_UNET_DTYPE\n",
        "            )[0]\n",
        "            GLOBAL_UNET_DEVICE = run_device\n",
        "        else:\n",
        "            log(\"üì¶ Reusing cached UNet.\")\n",
        "        unet = GLOBAL_UNET\n",
        "        mem_snapshot = log_memory(\"After UNet\", log)\n",
        "    except Exception as e:\n",
        "        log(f\"UNet error: {e}\")\n",
        "        hard_cleanup_models(log, keep_unet=False)\n",
        "        status_html = build_memory_status_html(mem_snapshot)\n",
        "        return [], \"\\n\".join(logs), status_html\n",
        "\n",
        "    # ----- CLIP -----\n",
        "    try:\n",
        "        if not selected_clip_name:\n",
        "            files = scan_clip_files(use_gguf_clip)\n",
        "            if not files:\n",
        "                raise RuntimeError(\"No CLIP models found in cache.\")\n",
        "            selected_clip_name = files[0]\n",
        "\n",
        "        if use_gguf_clip:\n",
        "            if not HAS_GGUF:\n",
        "                raise RuntimeError(\"GGUF requested but CLIPLoaderGGUF not available.\")\n",
        "            log(f\"üì¶ Loading GGUF CLIP: {selected_clip_name}\")\n",
        "            clip = CLIPLoaderGGUF().load_clip(selected_clip_name)[0]\n",
        "        else:\n",
        "            log(f\"üì¶ Loading CLIP: {selected_clip_name}\")\n",
        "            clip = CLIPLoader().load_clip(selected_clip_name, \"ltxv\")[0]\n",
        "\n",
        "        log(\"‚úÖ CLIP ready\")\n",
        "        mem_snapshot = log_memory(\"After CLIP\", log)\n",
        "    except Exception as e:\n",
        "        log(f\"CLIP error: {e}\")\n",
        "        hard_cleanup_models(log, keep_unet=False)\n",
        "        status_html = build_memory_status_html(mem_snapshot)\n",
        "        return [], \"\\n\".join(logs), status_html\n",
        "\n",
        "    # ----- LoRA -----\n",
        "    if use_lora and selected_lora not in (\"<none>\", None, \"\"):\n",
        "        try:\n",
        "            log(\n",
        "                f\"üéØ Applying LoRA: {selected_lora} \"\n",
        "                f\"(model={lora_strength_model}, clip={lora_strength_clip})\"\n",
        "            )\n",
        "            unet, clip = LoraLoader().load_lora(\n",
        "                unet,\n",
        "                clip,\n",
        "                selected_lora,\n",
        "                float(lora_strength_model),\n",
        "                float(lora_strength_clip),\n",
        "            )\n",
        "            log(\"‚úÖ LoRA applied.\")\n",
        "        except Exception as e:\n",
        "            log(f\"‚ö†Ô∏è LoRA failed: {e}\")\n",
        "\n",
        "    # ----- Text encode -----\n",
        "    try:\n",
        "        log(\"‚úè Encoding text...\")\n",
        "        pos = CLIPTextEncode().encode(clip, prompt)[0]\n",
        "        neg = CLIPTextEncode().encode(clip, negative or \"\")[0]\n",
        "        mem_snapshot = log_memory(\"After TextEncode\", log)\n",
        "    except Exception as e:\n",
        "        log(f\"‚ùå Text encoding failed: {e}\")\n",
        "        hard_cleanup_models(log, keep_unet=False)\n",
        "        status_html = build_memory_status_html(mem_snapshot)\n",
        "        return [], \"\\n\".join(logs), status_html\n",
        "\n",
        "    # Drop CLIP\n",
        "    try:\n",
        "        del clip\n",
        "    except Exception:\n",
        "        pass\n",
        "    gc.collect()\n",
        "    if use_cuda:\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # ----- Latent -----\n",
        "    log(f\"üåå Latent: {int(width)}√ó{int(height)}\")\n",
        "    latent = EmptyLatentImage().generate(int(width), int(height), 1)[0]\n",
        "\n",
        "    # ----- Sampling -----\n",
        "    log(f\"‚è≥ Sampling {int(steps)} steps cfg={float(cfg)}\")\n",
        "    try:\n",
        "        ctx = torch.autocast(\"cuda\") if use_cuda else contextlib.nullcontext()\n",
        "        with torch.inference_mode(), ctx:\n",
        "            samples = KSampler().sample(\n",
        "                model=unet,\n",
        "                seed=seed,\n",
        "                steps=int(steps),\n",
        "                cfg=float(cfg),\n",
        "                sampler_name=sampler_name,\n",
        "                scheduler=scheduler,\n",
        "                positive=pos,\n",
        "                negative=neg,\n",
        "                latent_image=latent,\n",
        "                denoise=1.0,\n",
        "            )[0]\n",
        "        mem_snapshot = log_memory(\"After Sampling\", log)\n",
        "    except Exception as e:\n",
        "        log(f\"Sampling failed: {e}\")\n",
        "        hard_cleanup_models(log, keep_unet=False)\n",
        "        status_html = build_memory_status_html(mem_snapshot)\n",
        "        return [], \"\\n\".join(logs), status_html\n",
        "\n",
        "    try:\n",
        "        del pos, neg, latent\n",
        "    except Exception:\n",
        "        pass\n",
        "    gc.collect()\n",
        "    if use_cuda:\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # ----- Decode -----\n",
        "    try:\n",
        "        log(\"üì¶ Loading VAE...\")\n",
        "        vae = VAELoader().load_vae(\"ae.safetensors\")[0]\n",
        "        log(\"üñº Decoding...\")\n",
        "        ctx = torch.autocast(\"cuda\") if use_cuda else contextlib.nullcontext()\n",
        "        with torch.inference_mode(), ctx:\n",
        "            decoded = VAEDecode().decode(vae, samples)[0]\n",
        "        mem_snapshot = log_memory(\"After Decode\", log)\n",
        "    except Exception as e:\n",
        "        log(f\"Decode failed: {e}\")\n",
        "        hard_cleanup_models(log, keep_unet=False)\n",
        "        status_html = build_memory_status_html(mem_snapshot)\n",
        "        return [], \"\\n\".join(logs), status_html\n",
        "\n",
        "    log(\"üß© Converting to images...\")\n",
        "    arr = decoded.cpu().numpy()\n",
        "    images = []\n",
        "    for i in range(arr.shape[0]):\n",
        "        img = (np.clip(arr[i], 0.0, 1.0) * 255).astype(np.uint8)\n",
        "        images.append(Image.fromarray(img))\n",
        "\n",
        "    try:\n",
        "        del vae, samples, decoded, arr\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    log(\"üßπ Final cleanup (keep UNet cached)...\")\n",
        "    hard_cleanup_models(log, keep_unet=True)\n",
        "    mem_snapshot = log_memory(\"After Cleanup\", log)\n",
        "\n",
        "    log(\"‚úÖ Done.\")\n",
        "    status_html = build_memory_status_html(mem_snapshot)\n",
        "    return images, \"\\n\".join(logs), status_html\n",
        "\n",
        "# ---------- Gradio UI ----------\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## üñº Z-Image Turbo ‚Äì LoRA + optional GGUF CLIP (cached UNet)\")\n",
        "\n",
        "    with gr.Row():\n",
        "        prompt = gr.Textbox(\n",
        "            label=\"Prompt\",\n",
        "            value=\"A detailed 512x512 illustration of a neon cyberpunk forest city at dusk\",\n",
        "            lines=3,\n",
        "        )\n",
        "        negative = gr.Textbox(\n",
        "            label=\"Negative prompt\",\n",
        "            value=\"low quality, blurry, distorted, watermark\",\n",
        "            lines=3,\n",
        "        )\n",
        "\n",
        "    with gr.Row():\n",
        "        steps = gr.Slider(1, 32, 8, step=1, label=\"Steps\")\n",
        "        cfg   = gr.Slider(0.0, 4.0, 1.0, step=0.1, label=\"CFG\")\n",
        "\n",
        "    with gr.Row():\n",
        "        sampler_name = gr.Dropdown(\n",
        "            [\"euler_ancestral\", \"euler\", \"dpmpp_2m\"],\n",
        "            value=\"euler\",\n",
        "            label=\"Sampler\",\n",
        "        )\n",
        "        scheduler = gr.Dropdown(\n",
        "            [\"normal\", \"karras\"],\n",
        "            value=\"normal\",\n",
        "            label=\"Scheduler\",\n",
        "        )\n",
        "        seed = gr.Number(-1, label=\"Seed (-1 = random)\")\n",
        "\n",
        "    # 1024 max res\n",
        "    with gr.Row():\n",
        "        width  = gr.Slider(256, 1024, 512, step=8, label=\"Width\")\n",
        "        height = gr.Slider(256, 1024, 512, step=8, label=\"Height\")\n",
        "\n",
        "    gr.Markdown(\"### CLIP / Text Encoder\")\n",
        "    with gr.Row():\n",
        "        use_gguf_clip = gr.Checkbox(False, label=\"Use GGUF CLIP (.gguf)\")\n",
        "        initial_clip_choices = scan_clip_files(False)\n",
        "        if \"qwen_3_4b.safetensors\" in initial_clip_choices:\n",
        "            initial_clip_value = \"qwen_3_4b.safetensors\"\n",
        "        elif initial_clip_choices:\n",
        "            initial_clip_value = initial_clip_choices[0]\n",
        "        else:\n",
        "            initial_clip_value = None\n",
        "\n",
        "        clip_dropdown = gr.Dropdown(\n",
        "            choices=initial_clip_choices,\n",
        "            value=initial_clip_value,\n",
        "            label=\"CLIP model file (cache/clip)\",\n",
        "        )\n",
        "\n",
        "    use_gguf_clip.change(\n",
        "        fn=update_clip_choices,\n",
        "        inputs=[use_gguf_clip],\n",
        "        outputs=[clip_dropdown],\n",
        "    )\n",
        "\n",
        "    gr.Markdown(\"### LoRA\")\n",
        "    with gr.Row():\n",
        "        use_lora = gr.Checkbox(False, label=\"Enable LoRA\")\n",
        "        lora_dropdown = gr.Dropdown(\n",
        "            choices=list_loras(),\n",
        "            value=\"<none>\",\n",
        "            label=\"LoRA file (Drive/loras)\",\n",
        "        )\n",
        "        refresh_loras_btn = gr.Button(\"üîÑ Refresh LoRAs\")\n",
        "\n",
        "    def refresh_loras():\n",
        "        return gr.update(choices=list_loras(), value=\"<none>\")\n",
        "\n",
        "    refresh_loras_btn.click(\n",
        "        fn=refresh_loras,\n",
        "        inputs=[],\n",
        "        outputs=[lora_dropdown],\n",
        "    )\n",
        "\n",
        "    with gr.Row():\n",
        "        lora_strength_model = gr.Slider(\n",
        "            -2.0, 2.0, 1.0, step=0.1,\n",
        "            label=\"LoRA Model Weight\",\n",
        "        )\n",
        "        lora_strength_clip = gr.Slider(\n",
        "            -2.0, 2.0, 0.0, step=0.1,\n",
        "            label=\"LoRA CLIP Weight\",\n",
        "        )\n",
        "\n",
        "    gr.Markdown(\"### ‚¨áÔ∏è LoRA Downloader (HuggingFace / Civitai)\")\n",
        "    with gr.Row():\n",
        "        lora_url = gr.Textbox(\n",
        "            label=\"LoRA URL\",\n",
        "            placeholder=\"https://civitai.com/api/download/models/...\",\n",
        "        )\n",
        "        lora_filename = gr.Textbox(\n",
        "            label=\"Save name (optional)\",\n",
        "            placeholder=\"my_lora_name\",\n",
        "        )\n",
        "    with gr.Row():\n",
        "        hf_token = gr.Textbox(\n",
        "            label=\"HF Token (optional)\",\n",
        "            type=\"password\",\n",
        "            placeholder=\"hf_...\",\n",
        "        )\n",
        "        civitai_token = gr.Textbox(\n",
        "            label=\"Civitai Token (optional)\",\n",
        "            type=\"password\",\n",
        "            placeholder=\"your civitai token...\",\n",
        "        )\n",
        "\n",
        "    download_lora_btn = gr.Button(\"üì• Download LoRA to Drive\")\n",
        "    lora_dl_log = gr.Textbox(\n",
        "        label=\"LoRA Downloader Log\",\n",
        "        lines=6,\n",
        "        interactive=False,\n",
        "    )\n",
        "\n",
        "    download_lora_btn.click(\n",
        "        fn=download_lora_from_ui,\n",
        "        inputs=[lora_url, lora_filename, hf_token, civitai_token],\n",
        "        outputs=[lora_dropdown, lora_dl_log],\n",
        "    )\n",
        "\n",
        "    run_btn = gr.Button(\"Generate üé®\")\n",
        "\n",
        "    gallery = gr.Gallery(\n",
        "        label=\"Result\",\n",
        "        height=512,\n",
        "        columns=2,\n",
        "    )\n",
        "\n",
        "    # Logs first\n",
        "    log_box = gr.Textbox(\n",
        "        label=\"Generation Logs\",\n",
        "        lines=18,\n",
        "        interactive=False,\n",
        "    )\n",
        "\n",
        "    # Then small memory HUD\n",
        "    mem_status = gr.HTML(\n",
        "        value=\"<span style='font-family:monospace;font-size:0.76rem;'>No memory data yet.</span>\",\n",
        "    )\n",
        "\n",
        "    run_btn.click(\n",
        "        fn=generate_image,\n",
        "        inputs=[\n",
        "            prompt, negative,\n",
        "            steps, cfg,\n",
        "            sampler_name, scheduler,\n",
        "            width, height,\n",
        "            seed,\n",
        "            use_lora, lora_dropdown,\n",
        "            lora_strength_model, lora_strength_clip,\n",
        "            use_gguf_clip, clip_dropdown,\n",
        "        ],\n",
        "        outputs=[gallery, log_box, mem_status],\n",
        "    )\n",
        "\n",
        "demo.launch(share=True, debug=True)"
      ],
      "metadata": {
        "id": "LV4jH3Yctw3I",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}